apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: mnist-pipeline
  annotations:
    tekton.dev/output_artifacts: '{"pre-process": [{"key": "artifacts/$PIPELINERUN/pre-process/X_test.tgz",
      "name": "pre-process-X_test", "path": "/tmp/outputs/X_test/data"}, {"key": "artifacts/$PIPELINERUN/pre-process/X_train.tgz",
      "name": "pre-process-X_train", "path": "/tmp/outputs/X_train/data"}, {"key":
      "artifacts/$PIPELINERUN/pre-process/X_val.tgz", "name": "pre-process-X_val",
      "path": "/tmp/outputs/X_val/data"}, {"key": "artifacts/$PIPELINERUN/pre-process/y_train.tgz",
      "name": "pre-process-y_train", "path": "/tmp/outputs/y_train/data"}, {"key":
      "artifacts/$PIPELINERUN/pre-process/y_val.tgz", "name": "pre-process-y_val",
      "path": "/tmp/outputs/y_val/data"}], "train": [{"key": "artifacts/$PIPELINERUN/train/model.tgz",
      "name": "train-model", "path": "/tmp/outputs/model/data"}]}'
    tekton.dev/input_artifacts: '{"evaluate": [{"name": "pre-process-X_test", "parent_task":
      "pre-process"}, {"name": "pre-process-X_val", "parent_task": "pre-process"},
      {"name": "pre-process-y_val", "parent_task": "pre-process"}, {"name": "train-model",
      "parent_task": "train"}], "train": [{"name": "pre-process-X_test", "parent_task":
      "pre-process"}, {"name": "pre-process-X_train", "parent_task": "pre-process"},
      {"name": "pre-process-X_val", "parent_task": "pre-process"}, {"name": "pre-process-y_train",
      "parent_task": "pre-process"}, {"name": "pre-process-y_val", "parent_task":
      "pre-process"}]}'
    tekton.dev/artifact_bucket: mlpipeline
    tekton.dev/artifact_endpoint: minio-service.kubeflow:9000
    tekton.dev/artifact_endpoint_scheme: http://
    tekton.dev/artifact_items: '{"evaluate": [], "pre-process": [["X_test", "$(workspaces.pre-process.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/X_test"],
      ["X_train", "$(workspaces.pre-process.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/X_train"],
      ["X_val", "$(workspaces.pre-process.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/X_val"],
      ["y_train", "$(workspaces.pre-process.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/y_train"],
      ["y_val", "$(workspaces.pre-process.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/y_val"]],
      "train": [["model", "$(workspaces.train.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/model"]]}'
    sidecar.istio.io/inject: "false"
    pipelines.kubeflow.org/big_data_passing_format: $(workspaces.$TASK_NAME.path)/artifacts/$ORIG_PR_NAME/$TASKRUN_NAME/$TASK_PARAM_NAME
    pipelines.kubeflow.org/pipeline_spec: '{"inputs": [{"default": "mnist-model",
      "name": "model_obc", "optional": true, "type": "String"}], "name": "Mnist Pipeline"}'
spec:
  params:
  - name: model_obc
    value: mnist-model
  pipelineSpec:
    params:
    - name: model_obc
      default: mnist-model
    tasks:
    - name: pre-process
      taskSpec:
        steps:
        - name: main
          args:
          - --train-path
          - data/train.csv
          - --test-path
          - data/test.csv
          - --X-train
          - $(workspaces.pre-process.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/X_train
          - --y-train
          - $(workspaces.pre-process.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/y_train
          - --X-val
          - $(workspaces.pre-process.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/X_val
          - --y-val
          - $(workspaces.pre-process.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/y_val
          - --X-test
          - $(workspaces.pre-process.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/X_test
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def _make_parent_dirs_and_return_path(file_path: str):
                import os
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                return file_path

            def pre_process(
                    train_path,
                    test_path,
                    X_train_file,
                    y_train_file,
                    X_val_file,
                    y_val_file,
                    X_test_file
            ):
                import numpy as np
                import pandas as pd
                from matplotlib import pyplot as plt
                import tensorflow as tf
                from tensorflow import keras
                import seaborn as sns

                from sklearn.model_selection import train_test_split
                from sklearn.pipeline import Pipeline
                from sklearn.compose import ColumnTransformer
                from sklearn.preprocessing import StandardScaler, MinMaxScaler
                from sklearn.base import BaseEstimator, TransformerMixin
                from sklearn.preprocessing import OneHotEncoder

                import pickle

                train_local_path = '/tmp/train.csv'
                test_local_path = '/tmp/test.csv'

                def save_pickle(object_file, target_object):
                    with open(object_file, "wb") as f:
                        pickle.dump(target_object, f)

                def init_s3_connection():
                    import boto3
                    from boto3 import session
                    import os
                    key_id = os.environ.get("AWS_ACCESS_KEY_ID")
                    secret_key = os.environ.get("AWS_SECRET_ACCESS_KEY")
                    bucket_name = os.environ.get("AWS_S3_BUCKET")
                    host = os.environ.get("AWS_S3_HOST")
                    port = os.environ.get("AWS_S3_PORT")
                    s3_endpoint = 'http://' + host + ":" + port
                    s3_client = boto3.client("s3", aws_access_key_id=key_id, aws_secret_access_key=secret_key, endpoint_url=s3_endpoint)
                    return s3_client

                s3_client = init_s3_connection()
                s3_client.download_file('rhods', train_path, train_local_path)
                s3_client.download_file('rhods', test_path, test_local_path)

                df_train = pd.read_csv(train_local_path)
                df_test = pd.read_csv(test_local_path)

                X = df_train.iloc[:,1:]
                y = df_train.iloc[:, 0]
                X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=15)

                class ReshapeFunc(BaseEstimator, TransformerMixin):
                    def __init__(self):
                        pass
                    def fit(self, X, y=None):
                        return self
                    def transform(self, X, y=None):
                        X = X.reshape((-1,28,28,1))
                        return X
                features_pipeline = Pipeline(steps=[
                    ('Normalize', MinMaxScaler()),
                    ('Reshape', ReshapeFunc())
                ])
                target_pipeline = Pipeline(steps=[
                    ('OneHot', OneHotEncoder())
                ])

                X_train = features_pipeline.fit_transform(X_train)
                y_train = target_pipeline.fit_transform(y_train.values.reshape(-1,1))
                y_train = y_train.toarray()
                X_val = features_pipeline.fit_transform(X_val)
                y_val = target_pipeline.fit_transform(y_val.values.reshape(-1, 1))
                y_val = y_val.toarray()
                X_test = features_pipeline.fit_transform(df_test)

                save_pickle(X_train_file, X_train)
                save_pickle(y_train_file, y_train)
                save_pickle(X_val_file, X_val)
                save_pickle(y_val_file, y_val)
                save_pickle(X_test_file, X_test)

            import argparse
            _parser = argparse.ArgumentParser(prog='Pre process', description='')
            _parser.add_argument("--train-path", dest="train_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--test-path", dest="test_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--X-train", dest="X_train_file", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--y-train", dest="y_train_file", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--X-val", dest="X_val_file", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--y-val", dest="y_val_file", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--X-test", dest="X_test_file", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = pre_process(**_parsed_args)
          env:
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                key: accesskey
                name: mlpipeline-minio-artifact
          - name: AWS_S3_HOST
            valueFrom:
              secretKeyRef:
                key: host
                name: mlpipeline-minio-artifact
          - name: AWS_S3_PORT
            valueFrom:
              secretKeyRef:
                key: port
                name: mlpipeline-minio-artifact
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                key: secretkey
                name: mlpipeline-minio-artifact
          - name: AWS_S3_BUCKET
            value: rhods
          - name: ORIG_PR_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['custom.tekton.dev/originalPipelineRun']
          image: quay.io/alegros/runtime-image:rhods-mnist-cpu
        - image: quay.io/alegros/busybox:latest
          name: output-taskrun-name
          command:
          - sh
          - -ec
          - echo -n "$(context.taskRun.name)" > "$(results.taskrun-name.path)"
        - image: quay.io/alegros/busybox:latest
          name: copy-results-artifacts
          command:
          - sh
          - -ec
          - |
            set -exo pipefail
            TOTAL_SIZE=0
            copy_artifact() {
            if [ -d "$1" ]; then
              tar -czvf "$1".tar.gz "$1"
              SUFFIX=".tar.gz"
            fi
            ARTIFACT_SIZE=`wc -c "$1"${SUFFIX} | awk '{print $1}'`
            TOTAL_SIZE=$( expr $TOTAL_SIZE + $ARTIFACT_SIZE)
            touch "$2"
            if [[ $TOTAL_SIZE -lt 3072 ]]; then
              if [ -d "$1" ]; then
                tar -tzf "$1".tar.gz > "$2"
              elif ! awk "/[^[:print:]]/{f=1} END{exit !f}" "$1"; then
                cp "$1" "$2"
              fi
            fi
            }
            copy_artifact $(workspaces.pre-process.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/X_train $(results.X-train.path)
            copy_artifact $(workspaces.pre-process.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/y_train $(results.y-train.path)
            copy_artifact $(workspaces.pre-process.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/X_val $(results.X-val.path)
            copy_artifact $(workspaces.pre-process.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/y_val $(results.y-val.path)
            copy_artifact $(workspaces.pre-process.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/X_test $(results.X-test.path)
          onError: continue
          env:
          - name: ORIG_PR_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['custom.tekton.dev/originalPipelineRun']
        results:
        - name: X-test
          type: string
          description: /tmp/outputs/X_test/data
        - name: X-train
          type: string
          description: /tmp/outputs/X_train/data
        - name: X-val
          type: string
          description: /tmp/outputs/X_val/data
        - name: taskrun-name
          type: string
        - name: y-train
          type: string
          description: /tmp/outputs/y_train/data
        - name: y-val
          type: string
          description: /tmp/outputs/y_val/data
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Pre process",
              "outputs": [{"name": "X_train"}, {"name": "y_train"}, {"name": "X_val"},
              {"name": "y_val"}, {"name": "X_test"}], "version": "Pre process@sha256=22485908e1a7233eecc88dde1c23d50d1ec8657fcf2df1a56065e952391bb903"}'
            tekton.dev/template: ''
        workspaces:
        - name: pre-process
      timeout: 525600m
      workspaces:
      - name: pre-process
        workspace: mnist-pipeline
    - name: train
      params:
      - name: pre-process-trname
        value: $(tasks.pre-process.results.taskrun-name)
      taskSpec:
        steps:
        - name: main
          args:
          - --X-train
          - $(workspaces.train.path)/artifacts/$ORIG_PR_NAME/$(params.pre-process-trname)/X_train
          - --y-train
          - $(workspaces.train.path)/artifacts/$ORIG_PR_NAME/$(params.pre-process-trname)/y_train
          - --X-val
          - $(workspaces.train.path)/artifacts/$ORIG_PR_NAME/$(params.pre-process-trname)/X_val
          - --y-val
          - $(workspaces.train.path)/artifacts/$ORIG_PR_NAME/$(params.pre-process-trname)/y_val
          - --X-test
          - $(workspaces.train.path)/artifacts/$ORIG_PR_NAME/$(params.pre-process-trname)/X_test
          - --model
          - $(workspaces.train.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/model
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n\
            \    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return\
            \ file_path\n\ndef train(\n        X_train_file,\n        y_train_file,\n\
            \        X_val_file,\n        y_val_file,\n        X_test_file,\n    \
            \    model_file\n):\n    import numpy as np\n    import pandas as pd\n\
            \    import tensorflow as tf\n    from tensorflow import keras\n    import\
            \ subprocess\n    import pickle\n\n    def save_pickle(object_file, target_object):\n\
            \        with open(object_file, \"wb\") as f:\n            pickle.dump(target_object,\
            \ f)\n\n    def load_pickle(object_file):\n        with open(object_file,\
            \ \"rb\") as f:\n            target_object = pickle.load(f)\n        return\
            \ target_object            \n\n    X_train = load_pickle(X_train_file)\n\
            \    y_train = load_pickle(y_train_file)\n    X_val = load_pickle(X_val_file)\n\
            \    y_val = load_pickle(y_val_file)\n    X_test = load_pickle(X_test_file)\n\
            \n    def build_model():\n        inp = keras.Input(shape=(28,28,1), name=\"\
            input_1\")\n        x = keras.layers.Conv2D(filters=32, kernel_size=(5,5),\
            \ strides=(1,1),padding='SAME', \n                                activation='relu')(inp)\n\
            \        x = keras.layers.MaxPool2D(pool_size=(2,2))(x)\n        x = keras.layers.BatchNormalization()(x)\n\
            \        x = keras.layers.Dropout(0.25)(x)\n        x = keras.layers.Conv2D(filters=64,\
            \ kernel_size=(5,5), padding='SAME', activation='relu')(x)\n        x\
            \ = keras.layers.MaxPool2D(pool_size=(2,2))(x)\n        x = keras.layers.BatchNormalization()(x)\n\
            \        x = keras.layers.Dropout(0.25)(x)\n        x = keras.layers.Flatten()(x)\n\
            \        x = keras.layers.Dense(256, activation='relu')(x)\n        x\
            \ = keras.layers.Dropout(0.5)(x)\n        output = keras.layers.Dense(10,\
            \ activation='softmax')(x)\n\n        model = keras.Model(inputs=inp,\
            \ outputs=output)\n        model.compile(\n            loss=keras.losses.CategoricalCrossentropy(),\n\
            \            optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n\
            \            metrics=['accuracy'])\n        return model, inp, output\n\
            \n    model, inp, out = build_model()\n    model.summary()\n\n    model.fit(X_train,\
            \ y_train, validation_data=(X_val, y_val), epochs=1, batch_size=32,\n\
            \                    callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss',mode='min',patience=10,\
            \ \n                                                            min_delta=0.005,\
            \ restore_best_weights=True),\n                            keras.callbacks.ReduceLROnPlateau(monitor\
            \ = 'val_loss', patience = 3)])\n    model_path_local = '/tmp/saved_model'\n\
            \    onnx_path_local = '/tmp/model.onnx'\n    tf.saved_model.save(model,\
            \ model_path_local)\n\n    cmd = 'python -m tf2onnx.convert --saved-model\
            \ ' + model_path_local + ' --output ' + onnx_path_local + ' --opset 13'\n\
            \n    proc = subprocess.run(cmd.split(), capture_output=True)\n    print(proc.returncode)\n\
            \    print(proc.stdout.decode('ascii'))\n    print(proc.stderr.decode('ascii'))\n\
            \n    def init_s3_connection():\n        import boto3\n        from boto3\
            \ import session\n        import os\n        key_id = os.environ.get(\"\
            AWS_ACCESS_KEY_ID\")\n        secret_key = os.environ.get(\"AWS_SECRET_ACCESS_KEY\"\
            )\n        bucket_name = os.environ.get(\"AWS_S3_BUCKET\")\n        host\
            \ = os.environ.get(\"AWS_S3_HOST\")\n        port = os.environ.get(\"\
            AWS_S3_PORT\")\n        s3_endpoint = 'http://' + host + \":\" + port\n\
            \        s3_client = boto3.client(\"s3\", aws_access_key_id=key_id, aws_secret_access_key=secret_key,\
            \ endpoint_url=s3_endpoint)\n        return s3_client\n\n    s3_client\
            \ = init_s3_connection()\n    bucket_name = \"rhods\"\n    s3_client.upload_file(onnx_path_local,\
            \ bucket_name, \"onnx/model-v2.onnx\")\n    save_pickle(model_file, model)\n\
            \nimport argparse\n_parser = argparse.ArgumentParser(prog='Train', description='')\n\
            _parser.add_argument(\"--X-train\", dest=\"X_train_file\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--y-train\", dest=\"\
            y_train_file\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--X-val\", dest=\"X_val_file\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--y-val\", dest=\"\
            y_val_file\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --X-test\", dest=\"X_test_file\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--model\", dest=\"model_file\", type=_make_parent_dirs_and_return_path,\
            \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
            \n_outputs = train(**_parsed_args)\n"
          env:
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                key: accesskey
                name: mlpipeline-minio-artifact
          - name: AWS_S3_HOST
            valueFrom:
              secretKeyRef:
                key: host
                name: mlpipeline-minio-artifact
          - name: AWS_S3_PORT
            valueFrom:
              secretKeyRef:
                key: port
                name: mlpipeline-minio-artifact
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                key: secretkey
                name: mlpipeline-minio-artifact
          - name: AWS_S3_BUCKET
            value: rhods
          - name: ORIG_PR_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['custom.tekton.dev/originalPipelineRun']
          image: quay.io/alegros/runtime-image:rhods-mnist-cpu
        - image: quay.io/alegros/busybox:latest
          name: output-taskrun-name
          command:
          - sh
          - -ec
          - echo -n "$(context.taskRun.name)" > "$(results.taskrun-name.path)"
        - image: quay.io/alegros/busybox:latest
          name: copy-results-artifacts
          command:
          - sh
          - -ec
          - |
            set -exo pipefail
            TOTAL_SIZE=0
            copy_artifact() {
            if [ -d "$1" ]; then
              tar -czvf "$1".tar.gz "$1"
              SUFFIX=".tar.gz"
            fi
            ARTIFACT_SIZE=`wc -c "$1"${SUFFIX} | awk '{print $1}'`
            TOTAL_SIZE=$( expr $TOTAL_SIZE + $ARTIFACT_SIZE)
            touch "$2"
            if [[ $TOTAL_SIZE -lt 3072 ]]; then
              if [ -d "$1" ]; then
                tar -tzf "$1".tar.gz > "$2"
              elif ! awk "/[^[:print:]]/{f=1} END{exit !f}" "$1"; then
                cp "$1" "$2"
              fi
            fi
            }
            copy_artifact $(workspaces.train.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/model $(results.model.path)
          onError: continue
          env:
          - name: ORIG_PR_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['custom.tekton.dev/originalPipelineRun']
        params:
        - name: pre-process-trname
        results:
        - name: model
          type: string
          description: /tmp/outputs/model/data
        - name: taskrun-name
          type: string
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Train", "outputs":
              [{"name": "model"}], "version": "Train@sha256=b595e399e543776d869a7563f6018c82d554fe9986205131d19a86de9f3b20e8"}'
            tekton.dev/template: ''
        workspaces:
        - name: train
      timeout: 525600m
      workspaces:
      - name: train
        workspace: mnist-pipeline
      runAfter:
      - pre-process
      - pre-process
      - pre-process
      - pre-process
      - pre-process
    - name: evaluate
      params:
      - name: train-trname
        value: $(tasks.train.results.taskrun-name)
      - name: pre-process-trname
        value: $(tasks.pre-process.results.taskrun-name)
      taskSpec:
        steps:
        - name: main
          args:
          - --X-val
          - $(workspaces.evaluate.path)/artifacts/$ORIG_PR_NAME/$(params.pre-process-trname)/X_val
          - --y-val
          - $(workspaces.evaluate.path)/artifacts/$ORIG_PR_NAME/$(params.pre-process-trname)/y_val
          - --X-test
          - $(workspaces.evaluate.path)/artifacts/$ORIG_PR_NAME/$(params.pre-process-trname)/X_test
          - --model
          - $(workspaces.evaluate.path)/artifacts/$ORIG_PR_NAME/$(params.train-trname)/model
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def evaluate(
                X_val_file,
                y_val_file,
                X_test_file,
                model_file
            ):
                import numpy as np
                import pandas as pd
                import pickle
                import tensorflow as tf
                from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

                from sklearn.metrics import confusion_matrix
                from keras import backend as K

                def load_pickle(object_file):
                    with open(object_file, "rb") as f:
                        target_object = pickle.load(f)
                    return target_object

                def init_s3_connection():
                    import boto3
                    from boto3 import session
                    import os
                    key_id = os.environ.get("AWS_ACCESS_KEY_ID")
                    secret_key = os.environ.get("AWS_SECRET_ACCESS_KEY")
                    bucket_name = os.environ.get("AWS_S3_BUCKET")
                    host = os.environ.get("AWS_S3_HOST")
                    port = os.environ.get("AWS_S3_PORT")
                    s3_endpoint = 'http://' + host + ":" + port
                    s3_client = boto3.client("s3", aws_access_key_id=key_id, aws_secret_access_key=secret_key, endpoint_url=s3_endpoint)
                    return s3_client

                X_val = load_pickle(X_val_file)
                y_val = load_pickle(y_val_file)
                X_test = load_pickle(X_test_file)
                model = load_pickle(model_file)

                # Precision (using keras backend)
                def precision_metric(y_true, y_pred):
                    threshold = 0.5  # Training threshold 0.5
                    y_pred_y = K.cast(K.greater(K.clip(y_pred, 0, 1), threshold), K.floatx())

                    true_positives = K.sum(K.clip(y_true * y_pred, 0, 1))
                    false_negatives = K.sum(K.clip(y_true * (1-y_pred), 0, 1))
                    false_positives = K.sum(K.clip((1-y_true) * y_pred, 0, 1))
                    true_negatives = K.sum(K.clip((1 - y_true) * (1-y_pred), 0, 1))

                    precision = true_positives / (true_positives + false_positives + K.epsilon())
                    return precision

                # Recall (using keras backend)
                def recall_metric(y_true, y_pred):
                    threshold = 0.5 #Training threshold 0.5
                    y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), threshold), K.floatx())

                    true_positives = K.sum(K.clip(y_true * y_pred, 0, 1))
                    false_negatives = K.sum(K.clip(y_true * (1-y_pred), 0, 1))
                    false_positives = K.sum(K.clip((1-y_true) * y_pred, 0, 1))
                    true_negatives = K.sum(K.clip((1 - y_true) * (1-y_pred), 0, 1))

                    recall = true_positives / (true_positives + false_negatives + K.epsilon())
                    return recall

                # F1-score (using keras backend)
                def f1_metric(y_true, y_pred):
                    precision = precision_metric(y_true, y_pred)
                    recall = recall_metric(y_true, y_pred)
                    f1 = 2 * ((precision * recall) / (recall+precision+K.epsilon()))
                    return f1

                y_val_pred = np.argmax(model.predict(X_val), axis=1)
                y_val_pred
                y_val_true = np.argmax(y_val,axis=1)
                y_val_true

                acc = accuracy_score(y_val_true, y_val_pred)
                f1_macro = f1_score(y_val_true, y_val_pred, average="macro")
                rec = recall_score(y_val_true, y_val_pred, average="macro")
                prec = precision_score(y_val_true, y_val_pred, average="macro")
                print(f'accuracy_score: {acc}')
                print(f'f1_score_macro: {f1_macro}')
                print(f'precision_score: {prec}')
                print(f'recall_score: {rec}')

            import argparse
            _parser = argparse.ArgumentParser(prog='Evaluate', description='')
            _parser.add_argument("--X-val", dest="X_val_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--y-val", dest="y_val_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--X-test", dest="X_test_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--model", dest="model_file", type=str, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = evaluate(**_parsed_args)
          env:
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                key: accesskey
                name: mlpipeline-minio-artifact
          - name: AWS_S3_HOST
            valueFrom:
              secretKeyRef:
                key: host
                name: mlpipeline-minio-artifact
          - name: AWS_S3_PORT
            valueFrom:
              secretKeyRef:
                key: port
                name: mlpipeline-minio-artifact
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                key: secretkey
                name: mlpipeline-minio-artifact
          - name: AWS_S3_BUCKET
            value: rhods
          - name: ORIG_PR_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['custom.tekton.dev/originalPipelineRun']
          image: quay.io/alegros/runtime-image:rhods-mnist-cpu
        params:
        - name: train-trname
        - name: pre-process-trname
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Evaluate", "outputs":
              [], "version": "Evaluate@sha256=08abe9a6139f5cff48462f49a58edc7cc6a274bf7a7b89c02473786906f996b3"}'
            tekton.dev/template: ''
        workspaces:
        - name: evaluate
      timeout: 525600m
      workspaces:
      - name: evaluate
        workspace: mnist-pipeline
      runAfter:
      - train
    workspaces:
    - name: mnist-pipeline
  timeouts:
    pipeline: 1051200m
    tasks: 525600m
  workspaces:
  - name: mnist-pipeline
    persistentVolumeClaim:
      claimName: ml-pipeline
