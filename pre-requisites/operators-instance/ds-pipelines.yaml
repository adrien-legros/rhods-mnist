---
apiVersion: kfdef.apps.kubeflow.org/v1
kind: KfDef
metadata:
  name: rhods-beta-features
  namespace: redhat-ods-applications
spec:
  applications:
    - kustomizeConfig:
        overlays:
          - metadata-store-mariadb
          - integration-odhdashboard
          - ds-pipeline-ui
          - component-visualization-server
          - object-store-minio
          - default-configs
        repoRef:
          name: manifests
          path: data-science-pipelines
      name: data-science-pipelines
  repos:
    - name: manifests
      uri: 'https://github.com/opendatahub-io/odh-manifests/tarball/v1.4.1'
---
apiVersion: dashboard.opendatahub.io/v1
kind: OdhApplication
metadata:
  name: data-science-pipelines
  namespace: redhat-ods-applications
spec:
  enable:
    actionLabel: Enable
    description: >-
      Clicking enable will add a card to the Enabled page to access the Data
      Science Pipelines interface.


      Before enabling, be sure you have installed OpenShift Pipelines and have
      an S3 Object store configured
    title: Enable Data Science Pipelines
    validationConfigMap: ds-pipelines-dashboardtile-validation-result
  img: >-
    <svg id="Layer_1" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg"
    viewBox="0 0 192
    145"><defs><style>.cls-1{fill:#e00;}</style></defs><title>RedHat-Logo-Hat-Color</title><path
    d="M157.77,62.61a14,14,0,0,1,.31,3.42c0,14.88-18.1,17.46-30.61,17.46C78.83,83.49,42.53,53.26,42.53,44a6.43,6.43,0,0,1,.22-1.94l-3.66,9.06a18.45,18.45,0,0,0-1.51,7.33c0,18.11,41,45.48,87.74,45.48,20.69,0,36.43-7.76,36.43-21.77,0-1.08,0-1.94-1.73-10.13Z"/><path
    class="cls-1"
    d="M127.47,83.49c12.51,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42l-7.45-32.36c-1.72-7.12-3.23-10.35-15.73-16.6C124.89,8.69,103.76.5,97.51.5,91.69.5,90,8,83.06,8c-6.68,0-11.64-5.6-17.89-5.6-6,0-9.91,4.09-12.93,12.5,0,0-8.41,23.72-9.49,27.16A6.43,6.43,0,0,0,42.53,44c0,9.22,36.3,39.45,84.94,39.45M160,72.07c1.73,8.19,1.73,9.05,1.73,10.13,0,14-15.74,21.77-36.43,21.77C78.54,104,37.58,76.6,37.58,58.49a18.45,18.45,0,0,1,1.51-7.33C22.27,52,.5,55,.5,74.22c0,31.48,74.59,70.28,133.65,70.28,45.28,0,56.7-20.48,56.7-36.65,0-12.72-11-27.16-30.83-35.78"/></svg>
  getStartedLink: >-
    https://access.redhat.com/documentation/en-us/red_hat_openshift_data_science/1
  betaTitle: Data Science Pipelines (beta)
  route: ds-pipeline-ui
  internalRoute: ds-pipeline-ui
  displayName: Data Science Pipelines
  kfdefApplications: []
  support: Red Hat managed
  beta: true
  provider: Red Hat
  docsLink: >-
    https://access.redhat.com/documentation/en-us/red_hat_openshift_data_science/1
  quickStart: create-data-science-pipeline
  getStartedMarkDown: >-
    # Getting Started With Data Science Pipelines

    Below are the list of samples that are currently running end to end taking
    the compiled Tekton yaml and deploying on a Tekton cluster directly. If you
    are interested more in the larger list of pipelines samples we are testing
    for whether they can be 'compiled to Tekton' format, please [look at the
    corresponding status
    page](https://github.com/red-hat-data-services/data-science-pipelines/tree/master/sdk/python/tests/README.md)

    [DSP Tekton User
    Guide](https://github.com/red-hat-data-services/data-science-pipelines/tree/master/guides/kfp-user-guide)
    is a guideline for the possible ways to develop and consume Data Science
    Pipelines. It's recommended to go over at least one of the methods in the
    user guide before heading into the KFP Tekton Samples.

    ## Prerequisites 

    - Install [OpenShift Pipelines
    Operator](https://docs.openshift.com/container-platform/4.7/cicd/pipelines/installing-pipelines.html).
    Then connect the cluster to the current shell with `oc` 


    - Install
    [kfp-tekton](https://github.com/red-hat-data-services/data-science-pipelines/tree/master/sdk/README.md)
    SDK

        ```
        # Set up the python virtual environment
        python3 -m venv .venv
        source .venv/bin/activate

        # Install the kfp-tekton SDK
        pip install kfp-tekton
        ```

    ## Samples

    - [MNIST End to End example with DSP
    components](https://github.com/red-hat-data-services/data-science-pipelines/tree/master/samples/e2e-mnist)


    - [Hyperparameter tuning using
    Katib](https://github.com/red-hat-data-services/data-science-pipelines/tree/master/samples/katib)


    - [Trusted AI Pipeline with AI Fairness 360 and Adversarial Robustness 360
    components](https://github.com/red-hat-data-services/data-science-pipelines/tree/master/samples/trusted-ai)


    - [Training and Serving Models with Watson Machine
    Learning](https://github.com/red-hat-data-service/sdata-science-pipelines/tree/master/samples/watson-train-serve#training-and-serving-models-with-watson-machine-learning)


    - [Lightweight python components
    example](https://github.com/red-hat-data-services/data-science-pipelines/tree/master/samples/lightweight-component)


    - [The flip-coin
    pipeline](https://github.com/red-hat-data-services/data-science-pipelines/tree/master/samples/flip-coin)


    - [Nested pipeline
    example](https://github.com/red-hat-data-services/data-science-pipelines/tree/master/samples/nested-pipeline)


    - [Pipeline with Nested
    loops](https://github.com/red-hat-data-services/data-science-pipelines/tree/master/samples/nested-loops)


    - [Using Tekton Custom Task on
    DSP](https://github.com/red-hat-data-services/data-science-pipelines/tree/master/samples/tekton-custom-task)


    - [The flip-coin pipeline using custom
    task](https://github.com/red-hat-data-services/data-science-pipelines/tree/master/samples/flip-coin-custom-task)


    - [Retrieve DSP run metadata using Kubernetes downstream
    API](https://github.com/red-hat-data-services/data-science-pipelines/tree/master/samples/k8s-downstream-api)
  description: >-
    Data Science Pipelines is a workflow platform with a focus on enabling
    Machine Learning operations such as Model development, experimentation,
    orchestration and automation.
  betaText: >-
    This beta application is available for early access prior to official
    release.
  category: Red Hat managed